{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipelines With Airflow\n",
    "\n",
    "\n",
    "__[1. Introduction](#first-bullet)__\n",
    "\n",
    "__[2. Calling An API In Python](#second-bullet)__\n",
    "\n",
    "__[3. Setting Up A PostgreSQL Database](#third-bullet)__\n",
    "\n",
    "__[4. Introduction To Airflow](#fourth-bullet)__\n",
    "\n",
    "__[5. An Example ETL Pipeline With Airflow](#fifth-bullet)__\n",
    "\n",
    "__[6. Debugging Airflow Code](#six-bullet)__\n",
    "\n",
    "__[7. Conclusion](#sevn-bullet)__\n",
    "\n",
    "\n",
    " \n",
    "## Introduction <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "\n",
    "In this blog post I want to go over the aspects of data engineering, sepecifically Extract, Transform, Load (ETL) operations and show how they can be automated and scheduled using <a href=\"https://airflow.incubator.apache.org/\">Apache Airflow</a>. You can see the source code for this project <a href=\"https://github.com/mdh266/AirflowDataPipeline\">here</a>.\n",
    "\n",
    "\n",
    "*Extracting* data can be done in a multitude of ways, but one of the most common is to query a <a href=\"https://en.wikipedia.org/wiki/Web_API\">WEB API</a>.  If the query is sucessful, then we will receive data back from the API and often times this data is in the form of <a href=\"https://en.wikipedia.org/wiki/JSON\">JSON</a>.  JSON can pretty much be thought of a semi-structured data or as a dictionary where the dictionary values are strings.  This means that the data must be *transformed* before being stored or *loaded* into a database. Airflow is a platform to schedule and monitor workflows and in this post I will show how to use it to extract the daily weather in New York from the <a href=\"https://openweathermap.org/api\">OpenWeatherMap</a> API, convert the temperature to Celsius and load the data in a simple <a href=\"https://www.postgresql.org/\">PostgreSQL</a> database.\n",
    "\n",
    "\n",
    "Let's first get started with how to query an API.\n",
    "\n",
    "\n",
    "## Calling An API In Python <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "\n",
    "To use a Web API to get data, you make a request to a remote web server, and retrieve the data you need. In Python, this is done using the <a href=\"http://docs.python-requests.org/en/master/\">requests</a> module. Below I wrote a module, <code>getWeather.py</code> that uses a GET request to obtain the weather for Brooklyn, NY. You can get a better feel for what the code is doing by checking out the OpenWeatherMap API documentation page <a href=\"https://openweathermap.org/current\">here</a>.\n",
    "\n",
    "![api](./images/api_call.png)\n",
    "\n",
    "\n",
    "Notice that I keep my API key in a seperate file called <code>config.py</code>.  In order to use this code yourself you would have to obtain your own API key and either place it in the code directly or have a variable <code>API_KEY = your-api-key </code> in a <code>config.py</code> file.  After the request has been made, I check to see if it was successful by checking the <code>status_code</code>, \n",
    "\n",
    "    result.status_code == 200\n",
    "   \n",
    "otherwise, I print an error.  Proper exception handling here is definitely something I will add in the future.  If the request is succesfull, then weather data is returned and is then dumped into a JSON file with a name that is the current date using the <a href=\"https://docs.python.org/2/library/json.html\">JSON package</a>.\n",
    "\n",
    "The above code is stored in a file title <code>getWeather.py</code> and be run from the command line by typing from the appropriate directory:\n",
    "\n",
    "    python getWeather.py\n",
    "    \n",
    "Note that this is the exact Bash command that I'll use to have Airflow collect daily weather data.  A great into into using API's with Python can be found <a href=\"https://www.dataquest.io/blog/python-api-tutorial/\">here</a>.  Now, let's go over how to set up a PostgreSQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up PostgreSQL Database <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "\n",
    "I went over the basics of how to use PostgresSQL in a previous blog <a href=\"http://michael-harmon.com/blog/SQLWars.html\">post</a>, so I'll just present the code I used to make one here.  The code below creates a table called <code>weather_table</code> in a local PostgreSQL database named <code>WeatherDB</code>.\n",
    "\n",
    "![api](./images/make_db.png)\n",
    "\n",
    "I only take a subset of the data that is returned from <a href=\"https://openweathermap.org/\">OpenWeatherMap</a>.  Specifically I transform and load the following into the database,\n",
    "\n",
    "- the city name\n",
    "- the country name\n",
    "- the latitude and longitude of the city\n",
    "- the date the API call was made\n",
    "- the humidity\n",
    "- the pressure\n",
    "- the minimum temperature of the day\n",
    "- the maximum temperature of the day\n",
    "- the current temperature\n",
    "- a description of the weather\n",
    "\n",
    "\n",
    "This script is stored in a file name <code>makeTable.py</code> and can be run using the command,\n",
    "\n",
    "    python makeTable.py\n",
    "\n",
    "From the appropriate directory and ** *before we set up our Airflow job* **.  Lastly, note that I don't have any password necessary to access the database, this was just for convience.  Now we can dive into Airflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction Airflow \n",
    "\n",
    "\n",
    "As mentioned in the introduction Airflow is a platform to schedule and monitor workflows and well be using it to set up a data pipeline. Data pipelines in Airflow are made up of DAGs (Directed Ayclic Graphs) that are scheduled to be completed at a specific times. Each node in the DAG will be a task that needs to be compeleted.  Taks that are dependent upon the completion of other tasks are run sequentially and tasks that are not dependent on one another can be run in parallel.  \n",
    "\n",
    "\n",
    "The main components of Airflow are \n",
    "\n",
    "- **Metadata DB  (database)** : Keeps track of tasks, how long each run took, etc.\n",
    "\n",
    "- **Webserver (Flask based UI)** : The webserver talks to metadata db to get information to present.\n",
    "\n",
    "- **Scheduler** : This scrolls the file system and puts things on the queue.\n",
    "\n",
    "- **Workers** : These do the actual tasks, these can can be separate from scheduler or the same. If they are separate then you can use celey: http://www.celeryproject.org/\n",
    "\n",
    "\n",
    "Airflow will dump all information about your DAGs into logs. The logs are going to be dumped to a file or database as well.  Just for simplicity I made a local directory in,\n",
    "\n",
    "    ~/airflow/logs\n",
    "    \n",
    "Notice the choice of directory to dump the logs this is decided by what <code>base_log_folder</code> is set to in the <code>airflow.cfg</code> file.  You can change it to store the logs remotely by setting the <code>remote_base_log_folder</code> variable in the <code>airflow.cfg</code> file.\n",
    "\n",
    "\n",
    "\n",
    "### Installing Airflow\n",
    "\n",
    "To install airflow first set your airflow home directy by typing into your terminal,\n",
    "\n",
    "    export AIRFLOW_HOME=<path_to_airflow_home>\n",
    "    \n",
    "I chose to set <code>AIRFLOW_HOME=~/airflow</code> which is the default setting.  Now we can install airflow with PostgreSQL using pip:\n",
    "    \n",
    "    pip install airflow[postgres]\n",
    "\n",
    "### Metadata DB\n",
    "\n",
    "We can then initialize the metadata database using,\n",
    "\n",
    "    airflow initdb \n",
    "    \n",
    "Out of the box, Airflow uses a sqlite database, which you should outgrow fairly quickly since no parallelization is possible using this database backend.  The default will be sqlite database in <code>AIRFLOW_HOME/airflow.db</code>. You can change the database choice using the <code>sql_alchemy_conn</code> variable in the <code>airflow.cfg</code> file.\n",
    "\n",
    "Airflow also by default works in conjunction with the SequentialExecutor which will only run task instances sequentially.  This is set in the executor variable in the <code>airflow.cfg</code> file.\n",
    "\n",
    "### Webserver\n",
    "We can start the webserver locally using the command,\n",
    "\n",
    "    airflow webserver -p 8080\n",
    "\n",
    "Then plug in http://0.0.0.0:8080/ into browser and you will get the Airflow UI. The webserver will be extemely helpful to understand what DAGS are running, how long they ran, etc. as well as setting up connections to databases as I will show later.\n",
    "\n",
    "### Scheduler\n",
    "\n",
    "The Airflow scheduler monitors all tasks and all DAGs, and triggers the task instances whose dependencies have been met. Behind the scenes, it monitors and stays in sync with a folder for all DAG objects it may contain, and periodically (every minute or so) inspects active tasks to see whether they can be triggered.\n",
    "The Airflow scheduler is designed to run as a persistent service in an Airflow production environment. To kick it off, all you need to do is execute airflow scheduler. It will use the configuration specified in <code>airflow.cfg</code>.\n",
    "\n",
    "    airflow scheduler\n",
    "    \n",
    "Note that if you run a DAG on a schedule_interval of one day, the run stamped 2016-01-01 will be trigger soon after 2016-01-01T23:59. In other words, the job instance is started once the period it covers has ended.\n",
    "\n",
    "### Workers\n",
    "In this example I won't be using any seperate workers since I'm running this on my personal computer.\n",
    "\n",
    "Let's now get into how to use Airflow to set up an ETL pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## An Example ETL Pipeline With Airflow <a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "\n",
    "Let's go over an example of an Airflow DAG to that calls the OpenWeatherMap API daily to get weather in Brooky, NY. and stores that data in the Postgres database we created.  The first thing we need to do is create a connection to the database (<code>postgres_conn_id</code>).  We do this by going the Airflow Webserver in aweb-browser and clicking on <code>Admin</code> tab and then choosing <code>connections</code> as show below:\n",
    "\n",
    "![api](./images/admin.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, click on the <code>create</code> link and enter the information relevent to create our Postgres connection. You can see what I did below,\n",
    "![api](./images/airflow_ui_db.png)\n",
    "\n",
    "\n",
    "We then click save and we can now use <code>weather_id</code> as our <code>postgres_conn_id</code> connnection id. \n",
    "\n",
    "Now, lets jump into creating our DAG; this will be stored in a python file we will call by convention the *DAG defintion file*.  In this file, the first thing we do is to import all the necessary libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![libs](./images/libraries.png)\n",
    "\n",
    "You can see that we import both the <a href=\"https://airflow.incubator.apache.org/code.html\">BashOperator</a> and <a href=\"https://airflow.incubator.apache.org/code.html\">PythonOperator</a>.  While DAGs describe how to run a workflow, operators determine what operations get done in what order.   DAG operators generally run independently of other operators unless there are dependencies.  Once an operator is instantiated, it will then be referred to as a *“task”*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the Python function that will transform and load our JSON data object into our database.  This can seen below as the function, <code>load_data</code>:\n",
    "\n",
    "![load](./images/load_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the first thing we do is to instantiate a <a href=\"\">PostgresHook</a> object and pass our postgres conection id, <code>weather_id</code>, to the constructor.  We then get the current day's date so what we can load the appropriate JSON data from the API request of this day.  Once we load the data, we can see that it is basically a dictionary of strings.  We then transform the values in this dictionary and check to make sure that the numerical values are not NaNs using <a href=\"http://www.numpy.org/\">NumPy<a>'s <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.isnan.html\">isnan</a> function.  If there are any NaNs in the numerical data we flag this data as invalid. \n",
    "\n",
    "We then cast all the individual data field values into a tuple which we then pass as a paramter along with the SQL insertion command, <code>insert_cmd</code>, into the PostgresHook object's run method.   The <code>run</code> method then inserts the data into the database.  **More checks on the validity and quality of our data would be better, but for the purposes of this blog post what we have done is sufficient. Also, note that as we have done things now, we enter one row into our database at a time, ideally we would load mutiple rows at once to be more efficent.**\n",
    "\n",
    "Now let's dive into a DAG defintion below,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DAGS](./images/DAGS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define our default parameters as a dictionary <code>default_paramter</code> and includes such information as the owner of the DAG and how many times, and how often to retry running the DAG if it fails.  Next we instantiate our dag in the command,\n",
    "\n",
    "    dag = DAG(dag_id=\"weatherDag\",\n",
    "              default_args=default_args,\n",
    "              start_date=datetime(2017,8,24),\n",
    "              schedule_interval=timedelta(minutes=1440))\n",
    "\n",
    "**The <code>dag_id</code> needs to be unique and we this will pass off to all the tasks which needs to be completed during their instantiation.**  Every DAG will have a <a href=\"https://docs.python.org/2/library/datetime.html\">datetime</a> object called the <code>start_date</code> which should be future as well as <a href=\"https://docs.python.org/2/library/datetime.html\">timedelta</a> object, <code>schedule_interval</code>, that dictates how often to run the DAG.  I set my DAG to run every 1440 minutes, i.e. everyday.\n",
    "\n",
    "\n",
    "Next we intantiate a BashOperator operator which becomes a task that executes the API call as discussed before:\n",
    "\n",
    "    task1 = BashOperator(task_id='get_weather',\n",
    "                        bash_command='python ~/airflow/dags/src/getWeather.py',\n",
    "                        dag=dag)\n",
    "\n",
    "\n",
    "Notice that we pass the DAG object in through the Operator constructor.  We note that **code>task_id</code>'s have to be unique within the DAG pipeline**.  We next instatiate a PythonOperator which will be <code>task2</code> that transformes and loads the JSON data that was pulled from API into the database:\n",
    "\n",
    "    task2 =  PythonOperator(task_id='transform_load',\n",
    "                            provide_context=True,\n",
    "                            python_callable=load_data,\n",
    "                            dag=dag)\n",
    "\n",
    "Notice that we pass the function, <code>load_data</code>, to the <code>python_callable</code> keyword parameter.\n",
    "\n",
    "Finally, we set the up pipline by saying one task depends on another being completed.  You can say that <code>task1</code> needs to be completed before <code>task2<code> can be started by using the following notatiot at the bottom of your <code>.py<code> script (DAG definition file):\n",
    "\n",
    "    task2.set_upstream(task1)\n",
    "\n",
    "or,\n",
    "\n",
    "    task1 >> task2\n",
    "\n",
    "Tasks which dont have dependencies between each other can be run concurrently.\n",
    "\n",
    "\n",
    "That's it! Pretty cool right?  The last thing I'll go over is some tricks I used to debug code while working with Airflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Airflow Codes <a class=\"anchor\" id=\"sixth-bullet\"></a>\n",
    "\n",
    "Debugging code is always trick and I found some useful tricks to help you figure out why your DAGs aren't working.  First you can see if there is Python syntax error by \"compiling it\"\n",
    "\n",
    "    python dag_def_.py\n",
    "\n",
    "You can then test an individual task by using,\n",
    "\n",
    "    airflow test <dag_id> <task_id> <todays date\n",
    "\n",
    "Can also do test the whole dag by doing a bag fill,\n",
    "\n",
    "\tAirflow backfill <dag_id> -s <todays_date> -e <todays_date>\n",
    "\n",
    "\n",
    "Sometims, you may need to delete the *.pyc* files and the DAGs themselves.  If you need to delete a dag, first delete the DAG data from the metadata_db:\n",
    "\n",
    "\tUse the UI -> Browse -> Dag Runs -> Then delete them all.\n",
    "\t\n",
    "Then you can delete DAGs by clearing the task instance states:\n",
    "\n",
    "    airflow clear <dag_id>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Airflow is an extremely useful tool building data pipelines and scheduling jobs in Python. It is pretty simple to use and in this post I went over a simple example how to perform ETL using Airflow.  There are definitely more things Airflow can do and I encourage you to learn more about.  For a great overview video of Airflow check out this <a href=\"https://www.youtube.com/watch?v=cHATHSB_450\">talk</a>.  For more detailed information on Airflow check out the documentation <a href=\"https://airflow.incubator.apache.org/\">page</a>, it's very clear and extensive."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
